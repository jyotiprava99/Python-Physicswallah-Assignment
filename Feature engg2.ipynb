{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb545ce-997a-4ed7-b72c-534e2fa87b5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Q1. **Min-Max scaling**:\n",
    "   - Min-Max scaling is a technique used to scale numerical features to a specific range, typically between 0 and 1.\n",
    "   - It works by subtracting the minimum value from each observation and then dividing by the range of the feature (i.e., the difference between the maximum and minimum values).\n",
    "   - This method is useful when the features have varying scales, and it helps to bring all features to a similar scale.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "     data = [[1], [5], [10], [15], [20]]\n",
    "\n",
    "     scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "     scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "     print(scaled_data)\n",
    "     ```\n",
    "\n",
    "Q2. **Unit Vector technique**:\n",
    "   - The Unit Vector technique scales each feature vector such that its length (magnitude) is equal to 1.\n",
    "   - It is commonly used in machine learning algorithms that rely on the Euclidean distance between data points, such as clustering algorithms.\n",
    "   - Unlike Min-Max scaling, which scales features to a specific range, the Unit Vector technique does not limit the range of the scaled features.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import Normalizer\n",
    "\n",
    "     data = [[1, 2], [3, 4], [5, 6]]\n",
    "\n",
    "     scaler = Normalizer()\n",
    "     scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "     print(scaled_data)\n",
    "     ```\n",
    "\n",
    "Q3. **PCA (Principal Component Analysis)**:\n",
    "   - PCA is a dimensionality reduction technique used to reduce the number of features in a dataset while preserving its essential information.\n",
    "   - It works by transforming the original features into a new set of orthogonal features called principal components, which are linear combinations of the original features.\n",
    "   - PCA identifies the directions (principal components) that maximize the variance in the data.\n",
    "   - Example:\n",
    "     ```python\n",
    "     from sklearn.decomposition import PCA\n",
    "\n",
    "     data = [[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]\n",
    "\n",
    "     pca = PCA(n_components=2)\n",
    "     reduced_data = pca.fit_transform(data)\n",
    "\n",
    "     print(reduced_data)\n",
    "     ```\n",
    "\n",
    "Q4. **Relationship between PCA and Feature Extraction**:\n",
    "   - PCA is a technique used for feature extraction, where new features (principal components) are derived from the original features.\n",
    "   - The principal components capture the maximum variance in the data, allowing for dimensionality reduction while retaining most of the information.\n",
    "   - PCA is used to transform the original feature space into a lower-dimensional feature space, effectively reducing the dimensionality of the dataset.\n",
    "   - Example:\n",
    "     ```python\n",
    "     # Continuing from the previous example\n",
    "     print(pca.components_)\n",
    "     ```\n",
    "Q5. **Using Min-Max scaling for preprocessing in a food delivery recommendation system**:\n",
    "   - In the food delivery recommendation system project, features such as price, rating, and delivery time may have different scales.\n",
    "   - To preprocess the data using Min-Max scaling:\n",
    "     - First, import the MinMaxScaler from sklearn.preprocessing.\n",
    "     - Then, apply Min-Max scaling to each feature individually to scale them between 0 and 1.\n",
    "     - This ensures that all features are on the same scale, making it easier for machine learning algorithms to learn from the data.\n",
    "\n",
    "Q6. **Using PCA for dimensionality reduction in predicting stock prices**:\n",
    "   - In the stock price prediction project, the dataset may contain numerous features related to company financial data and market trends.\n",
    "   - To reduce the dimensionality of the dataset using PCA:\n",
    "     - First, import PCA from sklearn.decomposition.\n",
    "     - Then, apply PCA to the dataset, specifying the desired number of principal components to retain.\n",
    "     - PCA will identify the principal components that capture the maximum variance in the data and transform the dataset into a lower-dimensional space.\n",
    "     - This reduces the number of features while retaining most of the information, making it easier to train predictive models.\n",
    "\n",
    "Q7. **Performing Min-Max scaling for a dataset**:\n",
    "   ```python\n",
    "   data = [1, 5, 10, 15, 20]\n",
    "   min_value = min(data)\n",
    "   max_value = max(data)\n",
    "   scaled_data = [((x - min_value) / (max_value - min_value)) * 2 - 1 for x in data]\n",
    "   print(scaled_data)\n",
    "   ```\n",
    "\n",
    "Q8. **Performing Feature Extraction using PCA**:\n",
    "   - In the given dataset with features like height, weight, age, gender, and blood pressure:\n",
    "     - Import PCA from sklearn.decomposition.\n",
    "     - Standardize the dataset if necessary.\n",
    "     - Apply PCA to the dataset, specifying the number of principal components to retain.\n",
    "     - Choose the number of principal components based on the explained variance ratio or by analyzing the cumulative variance explained by the components.\n",
    "     - Retain as many principal components as necessary to capture a significant portion of the variance in the data while reducing dimensionality.\n",
    "     - Example:\n",
    "       ```python\n",
    "       from sklearn.decomposition import PCA\n",
    "       from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "       data = [[...], [...], [...], ...]  # Your dataset\n",
    "       scaler = StandardScaler()\n",
    "       scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "       pca = PCA(n_components=3)  # Choose the number of components\n",
    "       reduced_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "       print(reduced_data)\n",
    "       ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f2764-234b-4736-83d8-e6a0e3cb095e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
